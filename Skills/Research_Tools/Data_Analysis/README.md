# Data Analysis

**ID:** `biomedical.research_tools.data_analysis`
**Version:** 1.0.0
**Status:** Production
**Category:** Research Tools / Data Analysis

---

## Overview

The **Data Analysis Skill** provides a comprehensive toolkit for statistical analysis, data manipulation, and visualization across biomedical research workflows. This skill enables AI agents to perform end-to-end data analysis tasks using industry-standard tools: **Python** (Pandas, NumPy), **R**, **SQL**, and leading visualization platforms (**Tableau**, **Power BI**).

This skill addresses the critical need for reproducible, scalable data analysis in life sciences research—from exploratory data analysis (EDA) to publication-ready visualizations and database querying.

---

## Key Capabilities

### 1. Python Data Analysis (Pandas, NumPy)

| Capability | Description | Use Case |
|------------|-------------|----------|
| **Data Wrangling** | Loading, cleaning, transforming tabular data | CSV/Excel/Parquet processing, missing value handling |
| **Statistical Computing** | Descriptive stats, hypothesis testing, correlations | Clinical trial endpoint analysis, biomarker comparisons |
| **Array Operations** | High-performance numerical computations | Genomic data matrices, image processing arrays |
| **DataFrame Manipulation** | Merging, grouping, pivoting, reshaping | Multi-omics integration, cohort stratification |

#### Core Python Libraries

| Library | Version | Purpose |
|---------|---------|---------|
| `pandas` | >=2.0.0 | Tabular data manipulation, I/O operations |
| `numpy` | >=1.24.0 | Numerical computing, array operations |
| `scipy` | >=1.10.0 | Statistical functions, scientific computing |
| `statsmodels` | >=0.14.0 | Statistical modeling, hypothesis testing |
| `scikit-learn` | >=1.3.0 | Machine learning, preprocessing, metrics |

### 2. R Statistical Computing

| Capability | Description | Use Case |
|------------|-------------|----------|
| **Statistical Modeling** | Linear/logistic regression, mixed models | Clinical outcome prediction, GWAS analysis |
| **Biostatistics** | Survival analysis, meta-analysis | Kaplan-Meier curves, forest plots |
| **Package Ecosystem** | Bioconductor, tidyverse integration | Genomic analysis, reproducible reporting |
| **Publication Graphics** | ggplot2 visualizations | Journal-ready figures |

#### Core R Packages

| Package | Purpose |
|---------|---------|
| `tidyverse` | Data manipulation (dplyr, tidyr, ggplot2) |
| `data.table` | High-performance data operations |
| `survival` | Survival analysis, Cox regression |
| `lme4` | Mixed-effects models |
| `Bioconductor` | Genomic data analysis ecosystem |

### 3. SQL Database Querying

| Capability | Description | Use Case |
|------------|-------------|----------|
| **Data Retrieval** | SELECT, JOIN, subqueries | EHR data extraction, cohort identification |
| **Aggregations** | GROUP BY, window functions | Patient summary statistics, longitudinal trends |
| **Data Integration** | Multi-table joins, CTEs | Cross-database linking, data warehouse queries |
| **Performance** | Query optimization, indexing strategies | Large-scale clinical databases |

#### Supported Database Systems

| Database | Driver | Common Use |
|----------|--------|------------|
| PostgreSQL | `psycopg2` | Research databases, OMOP CDM |
| MySQL/MariaDB | `pymysql` | Biobank repositories |
| SQLite | `sqlite3` | Local analysis, prototyping |
| SQL Server | `pyodbc` | Enterprise EHR systems |
| BigQuery | `google-cloud-bigquery` | Cloud-scale genomic data |

### 4. Data Visualization (Tableau, Power BI)

| Capability | Description | Use Case |
|------------|-------------|----------|
| **Interactive Dashboards** | Drill-down analytics, filters | Clinical trial monitoring, real-time metrics |
| **Automated Reporting** | Scheduled refreshes, alerts | Regulatory submissions, PI dashboards |
| **Data Connectors** | Live/extract connections | Multi-source data integration |
| **Collaborative Sharing** | Server publishing, embedded analytics | Stakeholder communication |

#### Visualization Outputs

| Platform | Output Format | Integration |
|----------|---------------|-------------|
| **Tableau** | .twbx workbooks, Tableau Server/Cloud | Python (TabPy), R integration |
| **Power BI** | .pbix reports, Power BI Service | Python visuals, R scripts |
| **Python (Matplotlib/Seaborn)** | PNG, SVG, PDF | Publication figures |
| **Plotly/Altair** | Interactive HTML | Web dashboards, notebooks |

---

## Technical Specifications

### Input Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `data_source` | `str` | Required | Path to file, database connection string, or URL |
| `analysis_type` | `str` | `"exploratory"` | Type: `exploratory`, `statistical`, `predictive`, `visualization` |
| `output_format` | `str` | `"dataframe"` | Output: `dataframe`, `json`, `csv`, `plot`, `report` |
| `language` | `str` | `"python"` | Analysis language: `python`, `r`, `sql` |
| `viz_tool` | `str` | `"matplotlib"` | Visualization: `matplotlib`, `plotly`, `tableau`, `powerbi` |

### Output Artifacts

| Output | Description |
|--------|-------------|
| Processed DataFrames | Cleaned, transformed datasets |
| Statistical Reports | Test results, confidence intervals, effect sizes |
| Visualizations | Publication-ready figures (PNG, SVG, HTML) |
| SQL Query Results | Tabular query outputs |
| Dashboard Definitions | Tableau/Power BI configuration files |

---

## Usage

### Python (Pandas/NumPy)

```python
import pandas as pd
import numpy as np
from scipy import stats

# Load and explore data
df = pd.read_csv("clinical_trial_data.csv")
print(df.describe())

# Data cleaning
df = df.dropna(subset=['primary_endpoint'])
df['log_biomarker'] = np.log1p(df['biomarker_value'])

# Statistical analysis
treatment = df[df['arm'] == 'treatment']['primary_endpoint']
control = df[df['arm'] == 'control']['primary_endpoint']
t_stat, p_value = stats.ttest_ind(treatment, control)

print(f"t-statistic: {t_stat:.3f}, p-value: {p_value:.4f}")

# Summary statistics by group
summary = df.groupby('arm').agg({
    'primary_endpoint': ['mean', 'std', 'count'],
    'biomarker_value': ['median', 'min', 'max']
})
```

### R Statistical Analysis

```r
library(tidyverse)
library(survival)

# Load data
data <- read_csv("patient_outcomes.csv")

# Survival analysis
surv_obj <- Surv(time = data$time_to_event, event = data$event_status)
km_fit <- survfit(surv_obj ~ treatment_arm, data = data)

# Cox proportional hazards
cox_model <- coxph(surv_obj ~ age + sex + treatment_arm, data = data)
summary(cox_model)

# Publication-ready Kaplan-Meier plot
library(survminer)
ggsurvplot(km_fit, data = data,
           risk.table = TRUE,
           pval = TRUE,
           conf.int = TRUE)
```

### SQL Database Queries

```sql
-- Cohort identification with eligibility criteria
WITH eligible_patients AS (
    SELECT
        patient_id,
        age,
        diagnosis_date,
        biomarker_value
    FROM patients p
    JOIN lab_results l ON p.patient_id = l.patient_id
    WHERE diagnosis_code IN ('C34.0', 'C34.1', 'C34.9')
      AND age BETWEEN 18 AND 75
      AND biomarker_value > 10.0
)
SELECT
    COUNT(*) as n_patients,
    AVG(age) as mean_age,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY biomarker_value) as median_biomarker
FROM eligible_patients;
```

### LLM Agent Integration (LangChain)

```python
from langchain.tools import tool
import pandas as pd
import numpy as np
from scipy import stats

@tool
def analyze_dataset(
    file_path: str,
    analysis_type: str = "descriptive",
    group_column: str = None
) -> str:
    """
    Performs automated data analysis on tabular datasets.

    Supports descriptive statistics, group comparisons, and correlation analysis.

    Args:
        file_path: Path to CSV, Excel, or Parquet file
        analysis_type: 'descriptive', 'comparative', or 'correlation'
        group_column: Column name for group comparisons (optional)

    Returns:
        Formatted analysis results as string
    """
    df = pd.read_csv(file_path)

    if analysis_type == "descriptive":
        result = df.describe().to_string()

    elif analysis_type == "comparative" and group_column:
        groups = df[group_column].unique()
        numeric_cols = df.select_dtypes(include=[np.number]).columns

        comparisons = []
        for col in numeric_cols[:5]:  # Limit to first 5 numeric columns
            group_data = [df[df[group_column] == g][col].dropna() for g in groups]
            if len(groups) == 2:
                t_stat, p_val = stats.ttest_ind(*group_data)
                comparisons.append(f"{col}: t={t_stat:.3f}, p={p_val:.4f}")

        result = "\n".join(comparisons)

    elif analysis_type == "correlation":
        corr_matrix = df.select_dtypes(include=[np.number]).corr()
        result = corr_matrix.to_string()

    else:
        result = df.describe().to_string()

    return f"Analysis Results ({analysis_type}):\n{result}"

@tool
def run_sql_query(connection_string: str, query: str) -> str:
    """
    Execute SQL query against a database and return results.

    Args:
        connection_string: Database connection string
        query: SQL SELECT query to execute

    Returns:
        Query results as formatted table
    """
    import sqlalchemy

    engine = sqlalchemy.create_engine(connection_string)
    df = pd.read_sql(query, engine)

    return f"Query returned {len(df)} rows:\n{df.to_string()}"

# Register tools with agent
tools = [analyze_dataset, run_sql_query]
```

### Tableau Integration (TabPy)

```python
# TabPy script for custom calculations in Tableau
import pandas as pd
import numpy as np
from scipy import stats

def survival_probability(time, event, group):
    """Calculate Kaplan-Meier survival probability for Tableau."""
    from lifelines import KaplanMeierFitter

    kmf = KaplanMeierFitter()
    results = []

    for g in set(group):
        mask = [x == g for x in group]
        t = [time[i] for i in range(len(time)) if mask[i]]
        e = [event[i] for i in range(len(event)) if mask[i]]

        kmf.fit(t, e, label=str(g))
        results.extend(kmf.survival_function_.values.flatten().tolist())

    return results
```

### Power BI Python Visual

```python
# Python script for Power BI custom visual
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# dataset is automatically provided by Power BI
df = dataset

# Create publication-quality figure
fig, ax = plt.subplots(figsize=(10, 6))
sns.boxplot(data=df, x='treatment_group', y='response_value', ax=ax)
ax.set_xlabel('Treatment Group', fontsize=12)
ax.set_ylabel('Response Value', fontsize=12)
ax.set_title('Treatment Response Distribution', fontsize=14)

plt.tight_layout()
plt.show()
```

---

## Methodology

### Statistical Best Practices

This skill follows established statistical methodologies:

| Analysis Type | Methodology | Reference |
|---------------|-------------|-----------|
| Hypothesis Testing | Neyman-Pearson framework, multiple testing correction | Benjamini & Hochberg (1995) |
| Effect Size | Cohen's d, Hedges' g, odds ratios | Cohen (1988) |
| Missing Data | Multiple imputation, sensitivity analysis | Rubin (1987) |
| Survival Analysis | Kaplan-Meier, Cox proportional hazards | Cox (1972) |

### Data Quality Checks

Automated validation includes:
- Missing value assessment and imputation strategies
- Outlier detection (IQR, Z-score, Mahalanobis distance)
- Data type validation and coercion
- Duplicate record identification
- Referential integrity checks for SQL joins

---

## Dependencies

### Python Environment

```
pandas>=2.0.0
numpy>=1.24.0
scipy>=1.10.0
statsmodels>=0.14.0
scikit-learn>=1.3.0
matplotlib>=3.7.0
seaborn>=0.12.0
plotly>=5.15.0
sqlalchemy>=2.0.0
pyarrow>=12.0.0
```

### R Environment

```r
install.packages(c(
    "tidyverse",
    "data.table",
    "survival",
    "survminer",
    "lme4",
    "ggplot2",
    "DBI",
    "RPostgres"
))

# Bioconductor packages
BiocManager::install(c("DESeq2", "limma", "edgeR"))
```

### Database Drivers

```bash
# PostgreSQL
pip install psycopg2-binary

# MySQL
pip install pymysql

# SQL Server
pip install pyodbc

# BigQuery
pip install google-cloud-bigquery
```

---

## Validation

This skill has been validated on:

- **Clinical Trial Datasets:** CDISC SDTM/ADaM format analysis
- **Electronic Health Records:** OMOP CDM queries (>1M patient records)
- **Genomic Data:** Gene expression matrices, variant call files
- **Real-World Evidence:** Claims databases, registry data

### Performance Benchmarks

| Dataset Size | Pandas Operation | R Operation | SQL Query |
|--------------|------------------|-------------|-----------|
| 100K rows | <1 second | <1 second | <2 seconds |
| 1M rows | ~5 seconds | ~8 seconds | ~10 seconds |
| 10M rows | ~45 seconds | ~60 seconds | ~30 seconds |

---

## Integration with Other Skills

This skill provides foundational analysis for:

- **Single-Cell RNA-seq QC:** Quality metric visualization and statistical thresholds
- **Clinical Note Summarization:** Structured data extraction and cohort analysis
- **Trial Eligibility Screening:** Patient cohort SQL queries and statistics
- **Drug Discovery:** ADMET property analysis and compound statistics
- **Precision Oncology:** Biomarker analysis and survival outcomes

---

## Tutorials

This skill includes **196+ tutorial files** from curated GitHub repositories:

### Python Tutorials (34 Jupyter Notebooks)

| Repository | Description | Source |
|------------|-------------|--------|
| **numpy-tutorials** | Official NumPy educational content | [numpy/numpy-tutorials](https://github.com/numpy/numpy-tutorials) |
| **pandas-workshop** | Conference workshop (ODSC, PyCon, SciPy 2025) | [stefmolin/pandas-workshop](https://github.com/stefmolin/pandas-workshop) |
| **pandas-cookbook** | Real-world examples with practical data | [jvns/pandas-cookbook](https://github.com/jvns/pandas-cookbook) |

**Key Notebooks:**
- `numpy-tutorials/content/tutorial-x-]ray.md` - X-ray image analysis
- `numpy-tutorials/content/tutorial-nlp-from-scratch.md` - NLP with NumPy
- `pandas-workshop/notebooks/1-getting_started_with_pandas.ipynb` - Pandas basics
- `pandas-workshop/notebooks/2-data_wrangling.ipynb` - Data wrangling
- `pandas-workshop/notebooks/3-data_visualization.ipynb` - Visualization with Pandas
- `pandas-cookbook/cookbook/Chapter 1 - Reading from a CSV.ipynb` - Data I/O

### R Tutorials (15 R Markdown Files)

| Repository | Description | Source |
|------------|-------------|--------|
| **tidyverse-workshop** | RStudio Education official workshop | [rstudio-education/welcome-to-the-tidyverse](https://github.com/rstudio-education/welcome-to-the-tidyverse) |
| **r-tidyverse-series** | Northwestern IT workshop series | [nuitrcs/r-tidyverse](https://github.com/nuitrcs/r-tidyverse) |

**Key Tutorials:**
- `tidyverse-workshop/01-Visualize-Data/01-Visualize-Data.Rmd` - ggplot2 basics
- `tidyverse-workshop/02-Transform-Data/02-Transform-Data.Rmd` - dplyr operations
- `tidyverse-workshop/03-Tidy-Data/03-Tidy-Data.Rmd` - tidyr reshaping
- `r-tidyverse-series/` - Complete workshop series with exercises

### SQL Tutorials (147 SQL Files)

| Repository | Description | Source |
|------------|-------------|--------|
| **biomedsql** | NIH biomedical text-to-SQL benchmark | [NIH-CARD/biomedsql](https://github.com/NIH-CARD/biomedsql) |
| **sql-for-analytics** | SQL for data analytics (Packt) | [TrainingByPackt/SQL-for-Data-Analytics](https://github.com/TrainingByPackt/SQL-for-Data-Analytics) |
| **healthcare-sql** | Healthcare dataset analysis | [SPARTANX21/SQL-Data-Analysis-Healthcare-Project](https://github.com/SPARTANX21/SQL-Data-Analysis-Healthcare-Project) |

**Key Resources:**
- `biomedsql/` - Scientific reasoning on biomedical knowledge bases
- `sql-for-analytics/Datasets/` - Real-world analytics datasets
- `healthcare-sql/` - Patient demographics, treatments, hospital analysis

### Visualization Tutorials (Tableau/Power BI)

| Repository | Description | Source |
|------------|-------------|--------|
| **tableau-powerbi-learning** | Hands-on learning path | [Ashleshk/TableaU-PowerBI-Visualisation-Learning-path](https://github.com/Ashleshk/TableaU-PowerBI-Visualisation-Learning-path) |
| **data-viz-projects** | MySQL, PostgreSQL, Tableau, Spark projects | [ptyadana/SQL-Data-Analysis-and-Visualization-Projects](https://github.com/ptyadana/SQL-Data-Analysis-and-Visualization-Projects) |

**Key Learning Paths:**
- Data cleaning and transformation workflows
- Dashboard design best practices
- Integration with Python/R for advanced analytics

### Tutorial Structure

```
tutorials/
├── python/
│   ├── numpy-tutorials/         # Official NumPy tutorials
│   ├── pandas-workshop/         # Conference workshop materials
│   └── pandas-cookbook/         # Practical examples
├── r/
│   ├── tidyverse-workshop/      # RStudio Education materials
│   └── r-tidyverse-series/      # Workshop series
├── sql/
│   ├── biomedsql/               # NIH biomedical SQL benchmark
│   ├── sql-for-analytics/       # Data analytics SQL
│   └── healthcare-sql/          # Healthcare analysis
└── visualization/
    ├── tableau-powerbi-learning/ # BI tool tutorials
    └── data-viz-projects/        # Visualization projects
```

---

## Files

| File | Description |
|------|-------------|
| `README.md` | This documentation |
| `data_analysis.yaml` | USDL skill definition |
| `tutorials/` | 196+ tutorial files (628 MB) |
| `python_tools.py` | Pandas/NumPy utility functions (planned) |
| `r_scripts/` | R analysis templates (planned) |
| `sql_templates/` | Common SQL query patterns (planned) |

---

## Compliance Considerations

### HIPAA/GDPR

- When querying clinical databases, ensure proper data access agreements
- De-identify data before exporting to visualization platforms
- Use secure database connections (SSL/TLS)
- Implement audit logging for data access

### Reproducibility

- Version control all analysis scripts
- Document software versions and random seeds
- Use containerization (Docker) for complex environments
- Export session info: `sessionInfo()` (R), `pip freeze` (Python)

---

## Related Skills

- **Biomni:** General-purpose biomedical agent with 150+ tools
- **BioMaster:** Multi-agent bioinformatics workflows
- **TrialGPT:** Clinical trial matching and analysis
- **LEADS:** Literature mining and systematic review

---

## Author

**MD BABU MIA**
*Artificial Intelligence Group*
*Icahn School of Medicine at Mount Sinai*
md.babu.mia@mssm.edu
