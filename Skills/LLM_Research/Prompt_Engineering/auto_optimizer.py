"""
Automatic Prompt Optimizer (APO) - 2026 Skills

A 'Meta-Learning' agent that improves prompts automatically.
Workflow:
1. Take an Initial Prompt & a Goal.
2. Generate variations (Mutations).
3. Evaluate outputs against the Goal (Fitness Function).
4. Select the best prompt.

This implements a basic 'Genetic Algorithm' for Prompt Engineering.
"""

import random
import time

class AutoPromptOptimizer:
    def __init__(self):
        self.history = []

    def mutate_prompt(self, base_prompt: str) -> list:
        """
        Generates variations of a prompt.
        Real implementation: Uses an LLM to 'rewrite this to be more concise/formal/creative'.
        """
        variations = [
            f"You are an expert. {base_prompt} Think step-by-step.",
            f"{base_prompt} Provide a concise, bulleted list.",
            f"Role: Senior Analyst. Task: {base_prompt} Focus on data."
        ]
        return variations

    def evaluate_output(self, output: str, expected_criteria: str) -> float:
        """
        Scores the LLM's output.
        Real implementation: Uses an 'Evaluator LLM' to grade the response 0-100.
        """
        score = 0
        if expected_criteria.lower() in output.lower():
            score += 50
        
        # Length heuristic (conciseness)
        if len(output) < 100:
            score += 20
            
        return score + random.randint(0, 10) # Noise

    def simulate_llm_call(self, prompt: str) -> str:
        """
        Mock LLM generation.
        """
        if "concise" in prompt.lower():
            return "Here is the data: 1. Trend Up, 2. Cost Down."
        elif "step-by-step" in prompt.lower():
            return "Step 1: Analyze. Step 2: Compute. Result: Trend is Up."
        else:
            return "The data indicates a generally positive trend with some cost reduction."

    def optimize(self, initial_prompt: str, goal: str, rounds=2):
        print(f"--- Starting Optimization for: '{initial_prompt}' ---")
        current_best = initial_prompt
        best_score = 0
        
        for r in range(rounds):
            print(f"\nRound {r+1}: Mutating...")
            candidates = self.mutate_prompt(current_best)
            
            round_best_prompt = None
            round_best_score = -1
            
            for cand in candidates:
                # 1. Generate
                output = self.simulate_llm_call(cand)
                
                # 2. Evaluate
                score = self.evaluate_output(output, goal)
                print(f"  Prompt: '{cand[:40]}...' -> Score: {score}")
                
                if score > round_best_score:
                    round_best_score = score
                    round_best_prompt = cand
            
            # Selection
            if round_best_score > best_score:
                best_score = round_best_score
                current_best = round_best_prompt
                print(f"  > New Champion found! (Score: {best_score})")
            else:
                print("  > No improvement this round.")
                
        return current_best, best_score

if __name__ == "__main__":
    optimizer = AutoPromptOptimizer()
    
    # User Goal
    base = "Summarize the sales report."
    criteria = "Trend"
    
    optimized_prompt, score = optimizer.optimize(base, criteria)
    
    print("\n" + "="*40)
    print("OPTIMIZATION COMPLETE")
    print(f"Best Prompt: {optimized_prompt}")
    print(f"Final Score: {score}")
