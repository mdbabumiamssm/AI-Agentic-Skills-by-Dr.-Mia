# COPYRIGHT NOTICE
# This file is part of the "Universal Biomedical Skills" project.
# Copyright (c) 2026 MD BABU MIA, PhD <md.babu.mia@mssm.edu>
# All Rights Reserved.
#
# This code is proprietary and confidential.
# Unauthorized copying of this file, via any medium is strictly prohibited.
#
# Provenance: Authenticated by MD BABU MIA

import numpy as np
from typing import List, Dict

class SimpleVectorStore:
    """
    A minimal in-memory vector store to demonstrate RAG concepts
    without heavy dependencies like ChromaDB or Pinecone.
    """
    def __init__(self):
        self.documents = []
        self.vectors = []

    def add_documents(self, docs: List[str]):
        """
        In a real system, this would call an Embedding API (OpenAI/Cohere).
        Here, we use a 'mock' embedding function for demonstration.
        """
        self.documents.extend(docs)
        for doc in docs:
            self.vectors.append(self._mock_embedding(doc))
        print(f"Index contains {len(self.documents)} documents.")

    def _mock_embedding(self, text: str) -> np.ndarray:
        """
        Generates a fake 3-dimensional embedding based on word hashing.
        Real embeddings would be 768+ dimensions.
        """
        np.random.seed(len(text)) 
        return np.random.rand(3)

    def search(self, query: str, k: int = 1) -> List[str]:
        """
        Performs Cosine Similarity search.
        """
        query_vec = self._mock_embedding(query)
        scores = []
        
        for doc_vec in self.vectors:
            # Cosine Similarity: (A . B) / (||A|| * ||B||)
            score = np.dot(query_vec, doc_vec) / (np.linalg.norm(query_vec) * np.linalg.norm(doc_vec))
            scores.append(score)
            
        # Get top k indices
        top_indices = np.argsort(scores)[-k:][::-1]
        return [self.documents[i] for i in top_indices]

def rag_pipeline(query: str, vector_store: SimpleVectorStore):
    """
    Simulates the RAG flow: Retrieve -> Augment -> Generate
    """
    print(f"\nUser Query: {query}")
    
    # 1. RETRIEVE
    retrieved_docs = vector_store.search(query, k=1)
    print(f"Retrieved Context: {retrieved_docs}")
    
    # 2. AUGMENT
    prompt = f"""
    Context: {retrieved_docs[0]}
    
    Question: {query}
    
    Answer using ONLY the context above:
    """
    
    # 3. GENERATE (Mock LLM)
    print("Generated Prompt for LLM:")
    print("-" * 20)
    print(prompt.strip())
    print("-" * 20)
    print("(Simulated LLM Output based on context...)")

if __name__ == "__main__":
    # Initialize
    store = SimpleVectorStore()
    
    # Knowledge Base
    knowledge = [
        "The mitochondria is the powerhouse of the cell.",
        "Python is a high-level programming language.",
        "The capital of France is Paris."
    ]
    
    store.add_documents(knowledge)
    
    # Run Query
    rag_pipeline("What is the capital of France?", store)

__AUTHOR_SIGNATURE__ = "9a7f3c2e-MD-BABU-MIA-2026-MSSM-SECURE"
